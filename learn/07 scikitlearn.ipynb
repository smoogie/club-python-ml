{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Scikit-learn\n",
    "\n",
    "This notebook only presents the general capabilities of the scikit-learn library. The goal is to understand when and for what the library can be used. Specific algorithms, metrics, and topics are discussed in separate notebooks, where we explain the theory behind them and practice their usage.\n",
    "\n",
    "\n",
    "Scikit-learn is a machine learning library in python built on NumPy, SciPy, and matplotlib. It provides a ready-to-use implementation of different ML algorithms for regression, classification, clustering, and building neural networks. It contains different validators and measurements to evaluate models, which helps in model selection and optimization. It also provides functionality for preprocessing (feature extraction and normalization), dimension reduction, and data visualization.\n",
    "\n",
    "Install and import scikit learn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6d69dc31df58803"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "!pip install scikit-learn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "265911b6a2051513",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As scikit-learn offers many features, we usually import specific algorithms/features or groups of them. Scikit-learn is divided by different functionalities like sklearn.linear_model, sklearn.svm, sklearn.semi_supervised, sklearn.neural_network, etc. This helps to find us what we need."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75be0dee9c315569"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example datasets\n",
    "\n",
    "Scikit-learn comes with ready-to-use example data set. We can get them with sklearn.datasets. It contains functions to load specific datasets. We have a few simple data sets and real data sets. Here is a list of things usefule for testing different models:\n",
    "- load_iris - good for classification, collection of iris classes with attributes\n",
    "- load_digits - good for classification, collection of handwritten digits\n",
    "- load_wine - good for classification, wine identification set\n",
    "- load_diabetes - good for regression, information about sickness progression\n",
    "- load_breast_cancer - good for classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72355ff2ab77afb0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print(iris.data)\n",
    "print(iris.feature_names)\n",
    "print(iris.target)\n",
    "print(iris.target_names)\n",
    "x, y = load_iris(return_X_y=True)\n",
    "print(x)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c916211dd8328a0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "SciKit Learn provides a few features to process data before using them for model training. There are a few main transformations that we will use often: \n",
    "- standardization - for example, scaling to a specific range or removing the mean and scaling to unit variance,  \n",
    "- Normalization - scaling individual samples to have unit norm\n",
    "- Binarization - to transform numeric values to boolean with defined thresholds\n",
    "encoding categorical features - change text/categorical features of data to numeric representation\n",
    "- Imputing missing values - to fill missing values using selected function to calculate missing values\n",
    "\n",
    "There are more data processing options.\n",
    "Different processing models and functions are available in ``sklearn.preprocessing``"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc73c762790cddbf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn import datasets, preprocessing\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X  = iris.data[:-10, :2]\n",
    "X_test = iris.data[-10:, :2]\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "standardized_X = scaler.transform(X)\n",
    "standardized_X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_test)\n",
    "print(standardized_X_test)\n",
    "\n",
    "binarizer = preprocessing.Binarizer(threshold=6.2).fit(X)\n",
    "binary_X = binarizer.transform(X_test)\n",
    "print(binary_X)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a794a87d35b04bb",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Often, we also want to split our data into training data sets and test data sets. This can be achieved with  ``sklearn.model_selection.train_test_split``. The function accepts features and labels that are sets of data with the same shape. Additionally, it accepts ``random_state`` to initiate randomization of the splitting, and  ``train_test_split`` defines how big should be the test set (percent/fraction of the whole data set)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3481b94b347acc0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wines = load_wine()\n",
    "X, y = wines.data[:, :0], wines.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=0)\n",
    "print(X.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6ff363d6db489c0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "13d5938095df360d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building and training model\n",
    "When data are prepared, we can train our models.\n",
    "\n",
    "To build the model, we just need to call a function that will create it. For example, we can use ``sklearn.linear_model.LinearRegression`` for linear regression, ``sklearn.svm.SVC`` for Support Vector Machines, ``sklearn.neighbors.KNeighborsClassifier`` for K nearest neighbors, ``sklearn.tree.DecisionTreeClassifier`` for decision tree, ``sklearn.linear_model.Lasso`` for lasso regression model, ``sklearn.cluster.KMeans`` for KMeans clustering and many more. Please check the documentation to review the attributes required by specific models. For example, KMeans expects ``n_clusters`` and ``random_state``.\n",
    "\n",
    "After we build the model, we need to train it with the ``.fit()`` function. For classification models, we need to pass features and labels; for clustering, only a set of features.\n",
    "\n",
    "After we train the model, we can use it for the prediction of values with ``.predict()`` function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ec44ca67f3e46e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try to train K nearest neighbors model for wine dataset  with 80% of original dataset and test it using 20% of original dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfcc786b754ef35"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import train_test_split, KNeighborsClassifier and load_wine function for dataset\n",
    "from sklearn.___ import ___\n",
    "from sklearn.___ import ___\n",
    "from sklearn.___ import ___\n",
    "\n",
    "# Import wine data with load_wine\n",
    "wines = ___\n",
    "# set labels to use classes names in results and features to train model\n",
    "labels = wines.target_names[___]\n",
    "features = wines.data\n",
    "\n",
    "# Split data to training and tests sets, where test size is 20%, use random_state=0\n",
    "X_train, X_test, y_train, y_test = ___.(___, ___, ___, random_state=0)\n",
    "\n",
    "# init KNeighborsClassifier with n_neighbors=6\n",
    "knn =___(___)\n",
    "# train model\n",
    "knn.__(___, ___)\n",
    "# predict results for test set\n",
    "prediction = knn.predict(___)\n",
    "# print prediction and y_test\n",
    "print(___)\n",
    "print(___)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78f24c717cb3b57c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try to train Linear Regression model for california housing dataset with 85% of original dataset and test it using 15% of original dataset. Print out first six target test data and prediction to compare them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6a3d2a493b72645"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import function to split train/test data, LinearRegression from linear_model and fetch_california_housing\n",
    "from ___ import ___\n",
    "from ___ import ___\n",
    "\n",
    "# Import california housing data\n",
    "cf_housing = ___()\n",
    "# Split data to training and tests sets, where test size is 15%\n",
    "___ = ___(___, ___, ___, random_state=10)\n",
    "\n",
    "# create linear regression model and train it\n",
    "lr = ___\n",
    "___\n",
    "# predict values for test dataset\n",
    "prediction = ___\n",
    "# print results\n",
    "print(prediction[0:6])\n",
    "print(y_test[0:6])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa39767e749d1c8e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "\n",
    "Scikit Learn provides different evaluation metrics for models. With them, we can evaluate the quality of our model.\n",
    "\n",
    "Usually, this is why we split our datasets into training data and test data. After we train our model, we use test data to evaluate it. If we are not satisfied with the results, we can adjust different model parameters or build a better dataset. We can find metrics in ``sklearn.metrics``\n",
    "\n",
    "For classification models, we usually use accuracy_score, classification_report, and confusion_matrix. For regression, we usually use: mean_absolute_error, mean_squared_error, r2_score. For clustering we can use: adjusted_rand_score, homogeneity_score, v_measure_score. Functions to calculate metrics accept target data and predicted data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad9772fb51d61a82"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    diabetes.data, diabetes.target, test_size=0.1, random_state=10)\n",
    "\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "prediction = regr.predict(X_test)\n",
    "print(\"R2-score: %.2f\" % r2_score(y_test , prediction) )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80ba0467fedae1d0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using Mean Absolute Error, Mean Squared Error and R2 Score, try to select the best regression model for diabetes data set (load_diabetes).\n",
    "\n",
    "Split origianl data to have 85% of train data, and 15% of test data.\n",
    "\n",
    "Check LinearRegression, Ridge, Lasso. For Ridge and Lasso use alpha=0.01"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f68024b7692a81"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import all what is needed\n",
    "from sklearn.metrics import ___\n",
    "from sklearn.linear_model import ___\n",
    "from sklearn.datasets import ___\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load dataset and split for test/train\n",
    "___\n",
    "___\n",
    "\n",
    "# Train linear regression and predict values\n",
    "___\n",
    "\n",
    "# Train ridge and predict values\n",
    "___\n",
    "\n",
    "# Train lasso and predict values\n",
    "___\n",
    "\n",
    "# print Mean Absolute Error, Mean Squared Error and R2 Score for models\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "350e3c205c3ec947",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sklearn also provides cross-validation. The ideas is to run our modeling process on different subsets of the data to get multiple measures of model quality. It is available as sklearn.model_selection.cross_val_score. We need to pass the model to evaluate, features to train, and target values (in that order). We often also pass cross-validation generator and scoring function. \n",
    "\n",
    "Try to compare KNeighborsClassifier and DecisionTreeClassifier using cross-validation. For cv pass KFold object with n_splits=8 and random_state=1. Test them with data wine dataset (load_wine function). The result should be presented in the box plot."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3ded139ad1462fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import all what is needed\n",
    "from sklearn.___ import ___\n",
    "from sklearn.___ import ___\n",
    "from sklearn.___ import ___\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = {\"KNN\": ___, \"Decision Tree Classifier\": ___}\n",
    "results = []\n",
    "\n",
    "wines = ___\n",
    "\n",
    "# Loop through the models' values\n",
    "for model in models.___:  \n",
    "  kf = KFold(n_splits=8, random_state=1, shuffle=True)\n",
    "  \n",
    "  # Perform cross-validation\n",
    "  cv_results = cross_val_score(___, ___, ___, cv=___)\n",
    "  results.append(cv_results)\n",
    "  \n",
    "plt.boxplot(results, labels=models.keys())\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a46b229ced17ce01",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine tuning models and pipelines\n",
    "\n",
    "Each model we use has hyperparameters that allow us to adjust/fine-tune the model. We define hyperparameters when we create a model before we train it. Scikit Learn provides functionality that helps us identify the best hyperparameters by testing selected models on provided data. We can use ``sklearn.model_selection.GridSearchCV``"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e172fe6049c8b437"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "x, y = load_diabetes(return_X_y=True)\n",
    "# Set up the parameter grid\n",
    "param_grid = {\"alpha\": np.linspace(0.0001, 1, 50)}\n",
    "lasso = Lasso()\n",
    "kf = KFold(n_splits=8, random_state=12, shuffle=True)\n",
    "# Instantiate lasso_cv\n",
    "lasso_cv = GridSearchCV(lasso, param_grid, cv=kf)\n",
    "# Fit to the training data\n",
    "lasso_cv.fit(x, y)\n",
    "print(\"Tuned lasso paramaters: {}\".format(lasso_cv.best_params_))\n",
    "print(\"Tuned lasso score: {}\".format(lasso_cv.best_score_))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b62535bb52fd8cc6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another often use feature is pipeline, available as ``sklearn.pipeline.Pipeline``. Pipeline accepts a list of steps - a sequence of data transformers with an optional final predictor."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7019f1bf5cff72dd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Create pipeline steps\n",
    "steps = [(\"scaler\", StandardScaler()),\n",
    "         (\"lasso\", Lasso(alpha=0.5))]\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=21)\n",
    "# Instantiate the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "pipeline.fit(X_train, y_train)\n",
    "#Â Calculate and print R-squared\n",
    "print(pipeline.score(X_test, y_test))\n",
    "print(pipeline.predict(X_train)[:10])\n",
    "print(y_train[:10])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ead4b69e4fc7ee9d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can mix pipelines and grid search together. In such a case, you need to pass the pipeline to theGridSearchCV instead of the model. Additionally, the name of the params for the grid should containe prefix with the name of the step with model, for example:\n",
    "``\n",
    "steps = [(\"lassostep\", Lasso())]\n",
    "pipeline = Pipeline(steps)\n",
    "param_grid = {\"lassostep__alpha\": np.linspace(0.0001, 1, 50)}\n",
    "``\n",
    "\n",
    "Build pipeline with SimpleImputer, StandardScaler and LogisticRegression. Use the iris dataset (load_iris) and split it into test and train data (20% test data); for data split, use random_state=21. Using GridSearchCV find the best solver and C hyperparameters. Test following solvers: \"newton-cg\", \"saga\", \"lbfgs\". To search C parameter use np.linspace(0.001, 1.0, 10). Display best params and score for fine tuned model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "429e8de908e5040"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import all what is needed (numpy, StandardScaler, SimpleImputer, LogisticRegression, Pipeline, etc.)\n",
    "___\n",
    "\n",
    "X, y = ___\n",
    "X_train, X_test, y_train, y_test = train_test_split(___)\n",
    "# Create steps\n",
    "steps = [___]\n",
    "\n",
    "# Set up pipeline\n",
    "pipeline = ___\n",
    "# define grid params\n",
    "params = {___}\n",
    "\n",
    "# Create the GridSearchCV object, train it and predict result for test set\n",
    "tuning = ___\n",
    "tuning.___\n",
    "y_pred = ___\n",
    "\n",
    "# Compute and print performance\n",
    "print(\"Tuned Logistic Regression Parameters: {}, Accuracy: {}\".format(tuning.best_params_, tuning.score(X_test, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1992de193a74178",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "Scikit-learn is the perfect library to start testing different types of models and evaluation metrics. It offers much more than what was presented in this notebook. With other notebooks, we will explore specific models, metrics, and data transformations, learning, and the theory behind them to better understand when and what should be used.\n",
    "\n",
    "\n",
    "The general flow for all models is to:\n",
    "1. Import data set\n",
    "1. Preprocess it to fit model requirements, for example, standardized, impune missing data\n",
    "1. Split data to test and train data ``train_test_split()``\n",
    "1. Train data ``.fit()``\n",
    "1. Evaluate model\n",
    "1. Use model\n",
    "For multistep processes, use pipelines. When you are not sure what hyperparameters to set - play with the model and fine-tune it (for example, with grid search).\n",
    "\n",
    "If you want to persist your models to not build it each time or import in selected projects/systems, you need to dump it to the file and load it again in the target location. For example, you can export your model with skl2onnx to onxx format, then use onnxruntime to run predictions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f4e50a0ccab6eae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
